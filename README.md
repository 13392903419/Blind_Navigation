# Blind Navigation - Tactile Paving Navigation Assistant System

<div align="center">

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Flask](https://img.shields.io/badge/Flask-3.0.0-green.svg)](https://flask.palletsprojects.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/wink-wink-wink555/blind_navigation.svg)](https://github.com/wink-wink-wink555/blind_navigation/stargazers)

</div>

> ğŸ“¹ Demo Video: https://www.bilibili.com/video/BV1kD57zGE68

## ğŸŒŸ Introduction

Blind Navigation is an innovative AI-powered navigation system designed for visually impaired individuals. It combines computer vision and artificial intelligence to identify tactile paving (guide paths) through real-time video analysis and provides intelligent voice guidance. The system also includes location sharing, AI map assistant, and other features to enhance travel safety and independence.

### Tech Stack

- **Frontend**: HTML5, CSS3, Vanilla JavaScript
- **Backend**: Flask (Python 3.8+)
- **AI Models**:
  - YOLO (You Only Look Once) - Tactile paving detection
  - Ollama (Qwen2.5:3b) - Personalized voice prompt generation
  - DeepSeek AI - Intelligent dialogue and route planning
- **Database**: MySQL
- **Third-party Services**:
  - Baidu Map API - Location services and route planning
  - Edge TTS / pyttsx3 - Text-to-speech synthesis

## ğŸ¯ Problems Solved

This system addresses the following challenges:

1. **Tactile Paving Recognition & Navigation**: Real-time video analysis to identify tactile paving position and direction changes, helping visually impaired individuals walk safely

2. **Real-time Voice Feedback**: Automatically provides personalized AI voice prompts when tactile paving direction changes are detected

3. **Intelligent Map Assistant**: Integrates DeepSeek AI and Baidu Map API for intelligent Q&A, location queries, route planning, and more

4. **Safety Monitoring**: Location sharing allows family members to remotely view the location of visually impaired individuals

5. **Personalized Experience**: Customizable voice speed, volume, address preferences, and other parameters

6. **Accessibility Design**: Reduces barriers for visually impaired individuals to use modern urban facilities

## âœ¨ Key Features

- ğŸ¥ **Real-time Video Analysis**: Uses YOLO model for real-time tactile paving detection
- ğŸ”Š **Intelligent Voice Feedback**: Uses Ollama (qwen2.5:3b) to generate personalized, context-aware voice prompts based on user profile (age, gender, name, preferences)
- ğŸ¤– **AI Map Assistant**: Natural language interaction using DeepSeek AI, supports location queries, route planning, nearby searches, etc.
- ğŸ‘¤ **User System**: Complete registration, login, and password recovery functionality
- ğŸ“ **Location Sharing**: Real-time location sharing for family members
- âš™ï¸ **Personalized Settings**: Customizable voice speed, volume, gender, age group, address preferences, etc.
- ğŸ¯ **Dual Mode**: Supports both visually impaired user mode and family member mode

### How Ollama Powers Navigation

When the system detects a change in tactile paving direction (left or right turn), it uses the Ollama qwen2.5:3b model to generate natural, personalized voice prompts. The AI considers:
- User's preferred name or nickname
- Age group (youth/middle-aged/senior) for appropriate tone
- Gender for voice selection
- Encouragement settings to provide motivational feedback
- Previous context to avoid repetitive messages

This creates a more human-like and engaging experience compared to static, pre-recorded messages.

## ğŸ“‹ Requirements

- Python 3.8+
- MySQL Database
- Ollama with qwen2.5:3b model installed
- YOLO model weights file (requires training or using provided model)
- Required Python libraries (see installation steps below)

## ğŸš€ Installation

### 1. Clone Repository

```bash
git clone https://github.com/wink-wink-wink555/blind_navigation.git
cd blind_navigation
```

### 2. Create Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

Or install packages individually:

```bash
pip install flask pymysql ultralytics ollama numpy pyttsx3 geopy Pillow edge-tts requests opencv-python Werkzeug
```

### 4. Install and Configure Ollama

Install Ollama and pull the qwen2.5:3b model:

```bash
# Visit https://ollama.com/ to download and install Ollama for your OS

# After installation, pull the qwen2.5:3b model
ollama pull qwen2.5:3b

# Verify the model is installed
ollama list
```

Make sure Ollama service is running on `http://localhost:11434` (default port).

### 5. Database Setup

Create MySQL database:

```sql
CREATE DATABASE blind_navigation CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
```

The application will automatically create required tables on first run.

### 6. Configuration

Modify configuration in `config.py` (copy from `config.example.py`):

- **Database Config** (`DB_CONFIG`): Set MySQL host, user, password, etc.
- **Email Config** (`EMAIL_CONFIG`): Configure QQ email SMTP service (for verification codes)
- **Baidu Map Config** (`BAIDU_MAP_CONFIG`): Set Baidu Map API key
- **DeepSeek AI Config** (`DEEPSEEK_CONFIG`): Set DeepSeek AI API key
- **YOLO Model Path** (`MODEL_WEIGHTS`): Set path to trained YOLO model weights

### 7. Prepare YOLO Model

- If you have a trained model: Configure the model weights file path in `config.py` under `MODEL_WEIGHTS`
- If you need to train a model: Use your own tactile paving dataset for training (based on ultralytics library)

## ğŸƒ Running the Application

```bash
python app.py
```

The application will run at http://127.0.0.1:5000/

## ğŸ“– Usage Guide

### 1. Account Management

#### Register Account
1. Visit system homepage and click "Register"
2. Fill in username, password, email, etc.
3. Click "Get Verification Code" - system will send code to your email
4. Enter received verification code to complete registration

#### Login
1. Enter username and password
2. Click "Login" to access the system
3. Click "Forgot Password" to reset if needed

### 2. Tactile Paving Navigation

#### Video Analysis
1. Click "Upload Video" and select video file for analysis
2. System automatically analyzes tactile paving in the video
3. Voice prompts play automatically when tactile paving direction changes are detected

#### Real-time Navigation
1. Mount your phone or tablet to ensure camera can capture the tactile paving ahead
2. Click "Start Navigation"
3. System analyzes camera feed in real-time and provides voice navigation guidance

### 3. AI Map Assistant

#### Using AI Assistant
1. Click "Map" tab on main interface
2. Enter your question in the input box, for example:
   - "Coordinates of Tiananmen Square"
   - "What convenience stores are near me?"
   - "How to get from Beijing Railway Station to Tiananmen Square?"
3. Click "Ask" button - AI assistant analyzes question and calls map services
4. System responds in natural language

### 4. Location Sharing

#### Share Location
1. Click "Location Sharing" button on main interface
2. Authorize system to access location information
3. Select family member account to share with
4. Click "Start Sharing"

#### View Location
1. Login with family member account
2. Click "View Location" on main interface
3. System displays map interface with real-time location marker

### 5. System Settings

#### Personalized Settings
1. Click "Settings" button on main interface
2. Adjust the following parameters:
   - **Gender**: Male/Female/Not specified
   - **Name**: Set preferred name or how you'd like to be addressed
   - **Age Group**: Youth/Middle-aged/Senior/Not specified
   - **Voice Speed**: Slow/Medium/Fast
   - **Voice Volume**: Low/Medium/High
   - **User Mode**: Visually impaired user/Family member
   - **Encouragement**: On/Off (provides encouragement when appropriate)
3. Click "Test Voice" to preview
4. Click "Save Settings" to save changes

## âš ï¸ Notes

- **Ollama service must be running** for personalized voice prompt generation
- Email configuration required for verification code functionality
- Model recognition quality depends on training data quality
- Ensure camera permissions are enabled when using camera
- Location sharing requires GPS permissions
- DeepSeek AI functionality requires valid API key
- Baidu Map functionality requires valid API key

## ğŸ“§ Contact

- **Email**: yfsun.jeff@gmail.com
- **GitHub**: [wink-wink-wink555](https://github.com/wink-wink-wink555)
- **LinkedIn**: [Yifei Sun](https://www.linkedin.com/in/yifei-sun-0bab66341/)
- **Bilibili**: [NO_Desire](https://space.bilibili.com/623490717)

## ğŸ™ Acknowledgments

Special thanks to the following members for their help with tactile paving dataset collection, annotation, and training:
- [Chen Xingyu](https://github.com/guangxiangdebizi)
- Wang Youyi
- Liu Yiheng
- Cai Yuxin
- Zhang Chenshu

## ğŸ“ Project Structure

```
blind_navigation/
â”œâ”€â”€ app.py                 # Flask application main file
â”œâ”€â”€ config.py              # Configuration file
â”œâ”€â”€ models/                # Database models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ database.py        # Database operations
â”œâ”€â”€ routes/                # Route blueprints
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth.py           # Authentication routes
â”‚   â”œâ”€â”€ main.py           # Main page routes
â”‚   â”œâ”€â”€ video.py          # Video processing routes
â”‚   â””â”€â”€ map.py            # Map-related routes
â”œâ”€â”€ services/              # Business services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ baidu_map_mcp.py  # Baidu Map services
â”‚   â””â”€â”€ deepseek_ai.py    # DeepSeek AI services
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ decorators.py     # Decorators
â”‚   â”œâ”€â”€ email_utils.py    # Email utilities
â”‚   â”œâ”€â”€ video_utils.py    # Video processing utilities
â”‚   â””â”€â”€ voice_utils.py    # Voice utilities
â”œâ”€â”€ templates/             # HTML templates
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ login.html
â”‚   â”œâ”€â”€ register.html
â”‚   â””â”€â”€ forget_password.html
â””â”€â”€ uploads/              # Upload directory
```

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

Copyright (c) 2025 wink-wink-wink555

You are free to use, modify, and distribute this software for personal or commercial purposes, provided that the copyright notice and permission notice are included in all copies.

For detailed terms, please refer to the [LICENSE](LICENSE) file.

---

â­ If this project helps you, please give it a star!
